{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB8bJddyoBhaC7Qq+y82Vm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anujushir/Ai-Bot-Projects/blob/main/Research_Paper_Summarizer_%26_Navigator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC4fFMYaDJxG"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "import re\n",
        "\n",
        "# ================== STYLING ==================\n",
        "st.set_page_config(page_title=\"Research Paper Summarizer\", layout=\"wide\")\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .section-header {\n",
        "        background-color: #f0f2f6;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "        margin: 10px 0px;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .paper-card {\n",
        "        border: 1px solid #ddd;\n",
        "        padding: 15px;\n",
        "        margin: 10px 0px;\n",
        "        border-radius: 5px;\n",
        "        background-color: white;\n",
        "    }\n",
        "    .nav-button {\n",
        "        background-color: #4CAF50;\n",
        "        color: white;\n",
        "        padding: 5px 10px;\n",
        "        border: none;\n",
        "        border-radius: 3px;\n",
        "        cursor: pointer;\n",
        "        margin: 2px;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ================== CONSTANTS ==================\n",
        "PROMPT_TEMPLATES = {\n",
        "    \"summary\": \"\"\"\n",
        "You are a research paper analysis expert. Analyze the given research paper and provide a structured summary with the following sections:\n",
        "\n",
        "1. **Title & Authors**: Extract the paper title and authors\n",
        "2. **Abstract**: Key summary of the paper\n",
        "3. **Key Contributions**: Main innovations or contributions\n",
        "4. **Methodology**: Approach and techniques used\n",
        "5. **Key Findings**: Main results and discoveries\n",
        "6. **Limitations**: Any limitations mentioned\n",
        "7. **Citations**: Important references cited\n",
        "\n",
        "For each major claim, idea, or metric mentioned, note the approximate section or page number where it appears.\n",
        "\n",
        "Paper Content: {document_content}\n",
        "\n",
        "Structured Summary:\n",
        "\"\"\",\n",
        "\n",
        "    \"section_analysis\": \"\"\"\n",
        "Analyze this specific section from a research paper and extract:\n",
        "- Main claims or hypotheses\n",
        "- Key metrics and results\n",
        "- Methodologies used\n",
        "- Important conclusions\n",
        "\n",
        "Section: {section_content}\n",
        "Page/Section Reference: {section_ref}\n",
        "\n",
        "Analysis:\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# ================== MODELS ==================\n",
        "EMBEDDING_MODEL = OllamaEmbeddings(model=\"deepseek-r1:1.5b\")\n",
        "DOCUMENT_VECTOR_DB = InMemoryVectorStore(EMBEDDING_MODEL)\n",
        "LANGUAGE_MODEL = OllamaLLM(model=\"deepseek-r1:1.5b\")\n",
        "\n",
        "# ================== DATA STRUCTURES ==================\n",
        "class ResearchPaper:\n",
        "    def __init__(self, title=\"\", authors=\"\", abstract=\"\", sections=None, summary=\"\"):\n",
        "        self.title = title\n",
        "        self.authors = authors\n",
        "        self.abstract = abstract\n",
        "        self.sections = sections if sections else {}\n",
        "        self.summary = summary\n",
        "        self.metadata = {}\n",
        "\n",
        "# ================== FUNCTIONS ==================\n",
        "def load_pdf_documents(file_path):\n",
        "    \"\"\"Load PDF documents\"\"\"\n",
        "    loader = PDFPlumberLoader(file_path)\n",
        "    return loader.load()\n",
        "\n",
        "def chunk_documents(raw_documents, chunk_size=800, chunk_overlap=100):\n",
        "    \"\"\"Split documents into chunks with section awareness\"\"\"\n",
        "    text_processor = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        add_start_index=True\n",
        "    )\n",
        "    return text_processor.split_documents(raw_documents)\n",
        "\n",
        "def extract_paper_sections(text):\n",
        "    \"\"\"Extract common research paper sections using pattern matching\"\"\"\n",
        "    sections = {}\n",
        "\n",
        "    # Common section headers pattern\n",
        "    patterns = {\n",
        "        'abstract': r'abstract|summary',\n",
        "        'introduction': r'introduction|^1\\.?\\s',\n",
        "        'methodology': r'method|approach|experiment|^2\\.?\\s',\n",
        "        'results': r'result|finding|^3\\.?\\s|^4\\.?\\s',\n",
        "        'discussion': r'discussion|conclusion|^5\\.?\\s|^6\\.?\\s',\n",
        "        'references': r'reference|bibliography'\n",
        "    }\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    current_section = 'header'\n",
        "    section_content = []\n",
        "\n",
        "    for line in lines:\n",
        "        line_lower = line.lower().strip()\n",
        "        section_found = False\n",
        "\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(pattern, line_lower) and len(line) < 200:  # Avoid long lines\n",
        "                if current_section in sections:\n",
        "                    sections[current_section] += '\\n'.join(section_content)\n",
        "                else:\n",
        "                    sections[current_section] = '\\n'.join(section_content)\n",
        "\n",
        "                current_section = section\n",
        "                section_content = []\n",
        "                section_found = True\n",
        "                break\n",
        "\n",
        "        if not section_found:\n",
        "            section_content.append(line)\n",
        "\n",
        "    # Add the last section\n",
        "    if current_section in sections:\n",
        "        sections[current_section] += '\\n'.join(section_content)\n",
        "    else:\n",
        "        sections[current_section] = '\\n'.join(section_content)\n",
        "\n",
        "    return sections\n",
        "\n",
        "def generate_structured_summary(paper_content):\n",
        "    \"\"\"Generate structured summary using LLM\"\"\"\n",
        "    conversation_prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATES[\"summary\"])\n",
        "    response_chain = conversation_prompt | LANGUAGE_MODEL\n",
        "    response = response_chain.invoke({\"document_content\": paper_content})\n",
        "    return response\n",
        "\n",
        "def analyze_section(section_content, section_ref):\n",
        "    \"\"\"Analyze a specific section\"\"\"\n",
        "    conversation_prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATES[\"section_analysis\"])\n",
        "    response_chain = conversation_prompt | LANGUAGE_MODEL\n",
        "    response = response_chain.invoke({\n",
        "        \"section_content\": section_content,\n",
        "        \"section_ref\": section_ref\n",
        "    })\n",
        "    return response\n",
        "\n",
        "def index_paper_chunks(paper_chunks):\n",
        "    \"\"\"Index paper chunks for semantic search\"\"\"\n",
        "    DOCUMENT_VECTOR_DB.add_documents(paper_chunks)\n",
        "\n",
        "def find_relevant_sections(query):\n",
        "    \"\"\"Find relevant sections using semantic search\"\"\"\n",
        "    return DOCUMENT_VECTOR_DB.similarity_search(query, k=3)\n",
        "\n",
        "# ================== MOCK DATA FOR DEMONSTRATION ==================\n",
        "def create_mock_papers():\n",
        "    \"\"\"Create mock research papers for demonstration\"\"\"\n",
        "    papers = []\n",
        "\n",
        "    # Mock paper 1\n",
        "    paper1 = ResearchPaper()\n",
        "    paper1.title = \"Attention Is All You Need: Transformer Architecture for NLP\"\n",
        "    paper1.authors = \"Vaswani et al.\"\n",
        "    paper1.abstract = \"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\"\n",
        "    paper1.sections = {\n",
        "        'abstract': \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks...\",\n",
        "        'introduction': \"Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches...\",\n",
        "        'methodology': \"The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder...\",\n",
        "        'results': \"We trained the base model for a total of 100,000 steps... The big model achieves a BLEU score of 28.4...\",\n",
        "        'discussion': \"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention...\"\n",
        "    }\n",
        "    paper1.summary = generate_structured_summary(\"\\n\".join(paper1.sections.values()))\n",
        "\n",
        "    papers.append(paper1)\n",
        "\n",
        "    return papers\n",
        "\n",
        "# ================== MAIN APP ==================\n",
        "st.title(\"üî¨ Research Paper Summarizer & Navigator\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# Sidebar for paper selection and filters\n",
        "st.sidebar.header(\"üìö Paper Selection\")\n",
        "\n",
        "# Topic input\n",
        "topic = st.sidebar.text_input(\"Enter research topic:\", placeholder=\"e.g., Transformers in NLP\")\n",
        "\n",
        "# Year range filter\n",
        "col1, col2 = st.sidebar.columns(2)\n",
        "with col1:\n",
        "    min_year = st.number_input(\"From year\", min_value=1990, max_value=2024, value=2018)\n",
        "with col2:\n",
        "    max_year = st.number_input(\"To year\", min_value=1990, max_value=2024, value=2024)\n",
        "\n",
        "# Venue filter\n",
        "venue = st.sidebar.selectbox(\"Venue filter:\", [\"All\", \"NeurIPS\", \"ICML\", \"ICLR\", \"CVPR\", \"ACL\", \"EMNLP\"])\n",
        "\n",
        "# Load/Create papers\n",
        "with st.spinner(\"Loading research papers...\"):\n",
        "    # In a real implementation, you would fetch papers based on topic\n",
        "    # For now, using mock data\n",
        "    research_papers = create_mock_papers()\n",
        "\n",
        "    # Index papers for search\n",
        "    all_chunks = []\n",
        "    for i, paper in enumerate(research_papers):\n",
        "        for section_name, content in paper.sections.items():\n",
        "            # Create document chunks for each section\n",
        "            from langchain.schema import Document\n",
        "            doc = Document(\n",
        "                page_content=content,\n",
        "                metadata={\n",
        "                    \"paper_index\": i,\n",
        "                    \"section\": section_name,\n",
        "                    \"title\": paper.title,\n",
        "                    \"authors\": paper.authors\n",
        "                }\n",
        "            )\n",
        "            all_chunks.append(doc)\n",
        "\n",
        "    # Index all chunks\n",
        "    index_paper_chunks(all_chunks)\n",
        "\n",
        "st.success(f\"‚úÖ Loaded {len(research_papers)} research papers!\")\n",
        "\n",
        "# Main content area\n",
        "tab1, tab2, tab3 = st.tabs([\"üìÑ Paper Browser\", \"üîç Semantic Search\", \"üìä Summary Dashboard\"])\n",
        "\n",
        "with tab1:\n",
        "    st.header(\"Paper Browser\")\n",
        "\n",
        "    for i, paper in enumerate(research_papers):\n",
        "        with st.expander(f\"üìñ {paper.title}\", expanded=i==0):\n",
        "            st.markdown(f\"**Authors:** {paper.authors}\")\n",
        "\n",
        "            # Section navigation\n",
        "            st.markdown('<div class=\"section-header\">üìë Quick Navigation</div>', unsafe_allow_html=True)\n",
        "            cols = st.columns(6)\n",
        "            sections_list = list(paper.sections.keys())\n",
        "\n",
        "            for idx, section in enumerate(sections_list):\n",
        "                with cols[idx % 6]:\n",
        "                    if st.button(section.title(), key=f\"nav_{i}_{section}\"):\n",
        "                        st.session_state[f\"selected_section_{i}\"] = section\n",
        "\n",
        "            # Display selected section or abstract by default\n",
        "            selected_section = st.session_state.get(f\"selected_section_{i}\", \"abstract\")\n",
        "            st.markdown(f'<div class=\"section-header\">üìñ {selected_section.upper()}</div>', unsafe_allow_html=True)\n",
        "            st.text_area(f\"Content\", paper.sections.get(selected_section, \"Content not available\"), height=200, key=f\"content_{i}_{selected_section}\")\n",
        "\n",
        "            # Section analysis\n",
        "            if st.button(f\"Analyze {selected_section}\", key=f\"analyze_{i}_{selected_section}\"):\n",
        "                with st.spinner(f\"Analyzing {selected_section}...\"):\n",
        "                    analysis = analyze_section(\n",
        "                        paper.sections.get(selected_section, \"\"),\n",
        "                        f\"Paper: {paper.title}, Section: {selected_section}\"\n",
        "                    )\n",
        "                    st.markdown(\"**Section Analysis:**\")\n",
        "                    st.write(analysis)\n",
        "\n",
        "with tab2:\n",
        "    st.header(\"Semantic Search Across Papers\")\n",
        "\n",
        "    search_query = st.text_input(\"üîç Search for concepts, methods, or findings:\")\n",
        "\n",
        "    if search_query:\n",
        "        with st.spinner(\"Searching across papers...\"):\n",
        "            relevant_docs = find_relevant_sections(search_query)\n",
        "\n",
        "        st.markdown(f\"**Found {len(relevant_docs)} relevant sections:**\")\n",
        "\n",
        "        for doc in relevant_docs:\n",
        "            paper_idx = doc.metadata[\"paper_index\"]\n",
        "            paper = research_papers[paper_idx]\n",
        "\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"paper-card\">\n",
        "                <h4>üìÑ {paper.title}</h4>\n",
        "                <p><strong>Section:</strong> {doc.metadata['section']}</p>\n",
        "                <p><strong>Relevance:</strong> {doc.page_content[:200]}...</p>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "            col1, col2 = st.columns([1, 4])\n",
        "            with col1:\n",
        "                if st.button(\"View Full Section\", key=f\"view_{paper_idx}_{doc.metadata['section']}\"):\n",
        "                    st.session_state[f\"selected_paper\"] = paper_idx\n",
        "                    st.session_state[f\"selected_section_{paper_idx}\"] = doc.metadata['section']\n",
        "                    st.rerun()\n",
        "            with col2:\n",
        "                if st.button(\"Analyze This Context\", key=f\"analyze_context_{paper_idx}_{doc.metadata['section']}\"):\n",
        "                    with st.spinner(\"Analyzing context...\"):\n",
        "                        analysis = analyze_section(doc.page_content, f\"From search: {search_query}\")\n",
        "                        st.markdown(\"**Context Analysis:**\")\n",
        "                        st.write(analysis)\n",
        "\n",
        "with tab3:\n",
        "    st.header(\"Paper Summaries Dashboard\")\n",
        "\n",
        "    for i, paper in enumerate(research_papers):\n",
        "        with st.expander(f\"üìä Summary: {paper.title}\", expanded=i==0):\n",
        "            if paper.summary:\n",
        "                st.markdown(paper.summary)\n",
        "            else:\n",
        "                with st.spinner(\"Generating summary...\"):\n",
        "                    full_content = \"\\n\".join(paper.sections.values())\n",
        "                    summary = generate_structured_summary(full_content)\n",
        "                    paper.summary = summary\n",
        "                    st.markdown(summary)\n",
        "\n",
        "# Download functionality\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.header(\"Export Results\")\n",
        "if st.sidebar.button(\"üì• Export Summaries as JSON\"):\n",
        "    export_data = []\n",
        "    for paper in research_papers:\n",
        "        export_data.append({\n",
        "            \"title\": paper.title,\n",
        "            \"authors\": paper.authors,\n",
        "            \"summary\": paper.summary,\n",
        "            \"sections\": list(paper.sections.keys())\n",
        "        })\n",
        "\n",
        "    st.sidebar.download_button(\n",
        "        label=\"Download JSON\",\n",
        "        data=json.dumps(export_data, indent=2),\n",
        "        file_name=\"research_paper_summaries.json\",\n",
        "        mime=\"application/json\"\n",
        "    )\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"*Note: This is a demonstration using mock data. In production, integrate with arXiv API, PubMed, or other academic databases.*\")"
      ]
    }
  ]
}